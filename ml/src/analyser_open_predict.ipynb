{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  687 predict:  1159\n"
     ]
    }
   ],
   "source": [
    "# This was used to train and read on the the open source sample code\n",
    "train_sections = []\n",
    "\n",
    "with open('../sample/sample.json', 'r') as f:\n",
    "  train_data = json.load(f)\n",
    "\n",
    "\n",
    "for file in train_data:\n",
    "    # print(file['name'], len(file['sections']))\n",
    "    for section in file['sections']:\n",
    "        train_sections.append(section['raw'])\n",
    "\n",
    "with open('../../data/cobols.json', 'r') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "sections = []\n",
    "importances = []\n",
    "\n",
    "for file in data:\n",
    "    # print(file['name'], len(file['sections']))\n",
    "    for section in file['sections']:\n",
    "        sections.append(section['raw'])\n",
    "\n",
    "print(\"train: \", len(train_sections), \"predict: \", len(sections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to 1938\n",
      "move 1595\n",
      "exit 1361\n",
      "map 1114\n",
      "ws 962\n",
      "send 928\n",
      "end 872\n",
      "exec 868\n",
      "3030 714\n",
      "perform 708\n",
      "thru 696\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.DataFrame({\"section\": train_sections}, columns=[\"section\"])\n",
    "data = pd.DataFrame({\"section\": sections}, columns=[\"section\"])\n",
    "\n",
    "regex_target = \"[^a-zA-z0-9\\s\\-]\" # include -\n",
    "# regex_target = \"[^a-zA-z0-9\\s]\" # exclude -\n",
    "\n",
    "train_data['section'] = train_data['section'].apply((lambda x: re.sub(regex_target,'',x)))\n",
    "data['section'] = data['section'].apply((lambda x: re.sub(regex_target,'',x)))\n",
    "\n",
    "max_fatures = 2000\n",
    "tokenizer =  Tokenizer(num_words=max_fatures, split=' ', filters='-!\"#$%&()*+,./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(train_data['section'].values)\n",
    "X = tokenizer.texts_to_sequences(data['section'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "\n",
    "# data\n",
    "for i in range(11):\n",
    "    word = list(tokenizer.word_index.keys())[i]\n",
    "    count = tokenizer.word_counts[word]\n",
    "    print(word, count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../model/model_open')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 1s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "predits = model.predict(X)\n",
    "\n",
    "important_predicts = []\n",
    "\n",
    "for i in range(len(predits)):\n",
    "    item = predits[i]\n",
    "    value = item[1]\n",
    "    if value > 0.5:\n",
    "        important_predicts.append((True, value))\n",
    "    else:\n",
    "        important_predicts.append((False, 1 - value))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "\n",
    "with open('../../data/cobols.json', 'r') as f:\n",
    "  data = json.load(f)\n",
    "for file in data:\n",
    "    # print(file['name'], len(file['sections']))\n",
    "    for section in file['sections']:\n",
    "        section['importance'] = important_predicts[i][0]\n",
    "        section['confidence'] = float(important_predicts[i][1])\n",
    "        i += 1\n",
    "\n",
    "with open('../../data/cobols_predicts.json', 'w') as f:\n",
    "    json.dump(data, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('ml': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4cd2858a4c83a66b978afaceafd2aa0fa6b56b94a4c0d049b30d36a609f6e80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
